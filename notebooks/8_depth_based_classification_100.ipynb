{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(930525)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "warnings.simplefilter('once')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "url = requests.get('https://docs.google.com/spreadsheets/d/1KO_wGiEagJ8PMO2BzSDI1IXHYO4RHZMMSWXlT48peiQ/export?format=csv')\n",
    "csv_raw = StringIO(url.text)\n",
    "df_truth = pd.read_csv(csv_raw)\n",
    "\n",
    "inf_tax_file = \"/mnt/btrfs/data/gtdb_95/gtdb_genomes_reps_r95/r95.gtdb.tax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tax = pd.read_csv(inf_tax_file, names=[\"assembly_accession\", \"tax\"], sep=\"\\t\")\n",
    "\n",
    "df_tax[\"species\"] = [\";\".join(_.split(\";\")[:7]) for _ in df_tax.tax]\n",
    "df_tax[\"genus\"] = [\";\".join(_.split(\";\")[:6]) for _ in df_tax.tax]\n",
    "df_tax[\"family\"] = [\";\".join(_.split(\";\")[:5]) for _ in df_tax.tax]\n",
    "\n",
    "accession_to_genus = dict()\n",
    "for t in df_tax.itertuples():\n",
    "    accession_to_genus[t.assembly_accession] = t.genus.split(\";\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "files = glob(f\"/mnt/btrfs/data/type_1/species_mc/b6_capitalist_split_by_sample/*.{depth}.extra.csv\")\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    name = '_'.join(os.path.basename(file).split('.')[:-4])\n",
    "    df = pd.read_csv(file, index_col = 0)\n",
    "    df['dataset'] = name\n",
    "    dfs.append(df)\n",
    "df_type_1_features = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the true species accessions\n",
    "dd = defaultdict(set)\n",
    "\n",
    "dd_genus = defaultdict(set)\n",
    "for group, df in df_truth.groupby('dataset'):\n",
    "    mask_nan = df_truth['database_accession'].astype(str) == 'nan'\n",
    "    \n",
    "    for row in df.loc[mask_nan].itertuples():\n",
    "        # get the genus of the nans\n",
    "        dd_genus[group].add(\"g__\" + row.name.replace(\"_\", \" \").split()[0])\n",
    "        dd_genus[group].add(\"g__\" + row.homotypic_synonym.replace(\"_\", \" \").split()[0])\n",
    "    \n",
    "    dd[group] = set(df.loc[~mask_nan, \"database_accession\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_1_features.groupby('dataset').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, t in df_type_1_features.iterrows():\n",
    "    if t['assembly_accession'] in dd[t['dataset']]:\n",
    "        rows.append(True)\n",
    "    else:\n",
    "        rows.append(False)\n",
    "df_type_1_features[\"truth\"] = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_1_features['join'] = df_type_1_features['tree_closest_assembly_accession'] + \"_\" + df_type_1_features['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['hits',\n",
    " 'percent_coverage',\n",
    " 'mean_coverage',\n",
    " 'sd_coverage',\n",
    " 'percent_binned_coverage',\n",
    " 'mean_binned_coverage',\n",
    " 'sd_binned_coverage',\n",
    " 'expected_percent_coverage',\n",
    " 'shannon_entropy',\n",
    " 'percent_max_uncovered_region',\n",
    " 'largest_pileup',\n",
    " 'largest_binned_pileup',\n",
    " 'gc_content',\n",
    " 'total_genome_length',\n",
    " 'ungapped_genome_length',\n",
    " 'num_n_groups',\n",
    " 'consecutive_ns',\n",
    " 'tree_dist',\n",
    " 'tree_top_dist',\n",
    " 'gf_checkm_completeness',\n",
    " 'gf_checkm_contamination',\n",
    " 'relative_abundance',\n",
    " 'tree_hits',\n",
    " 'tree_percent_coverage',\n",
    " 'tree_mean_coverage',\n",
    " 'tree_sd_coverage',\n",
    " 'tree_percent_binned_coverage',\n",
    " 'tree_mean_binned_coverage',\n",
    " 'tree_sd_binned_coverage',\n",
    " 'tree_expected_percent_coverage',\n",
    " 'tree_shannon_entropy',\n",
    " 'tree_percent_max_uncovered_region',\n",
    " 'tree_largest_pileup',\n",
    " 'tree_largest_binned_pileup',\n",
    " 'tree_dist',\n",
    " 'tree_top_dist'\n",
    "]\n",
    "\n",
    "# features = [\n",
    "#  'gf_checkm_completeness',\n",
    "#  'gf_checkm_contamination',\n",
    "#  'tree_relative_abundance'\n",
    "# ]\n",
    "\n",
    "# features = ['relative_abundance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_1_features['relative_abundance'] = df_type_1_features['hits'] / df_type_1_features.groupby('dataset')['hits'].transform('sum')\n",
    "\n",
    "df_type_1_features.reset_index(inplace=True, drop=True)\n",
    "df_type_1_features['dataset_cat'] = pd.Series([_.split(\"_\")[0] for _ in df_type_1_features['dataset']], dtype='category')\n",
    "\n",
    "categories = df_type_1_features['dataset_cat'].cat.categories\n",
    "\n",
    "X = df_type_1_features[features + [\"assembly_accession\", \"dataset\", \"truth\", \"dataset_cat\"]]\n",
    "\n",
    "# X = X.replace([np.inf, -np.inf], np.nan)\n",
    "# X = X.loc[:, features].dropna()\n",
    "\n",
    "cv = PredefinedSplit(X['dataset_cat'].cat.codes)\n",
    "X = X.loc[:, features].copy()\n",
    "X.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y = df_type_1_features.loc[:, \"truth\"]\n",
    "y.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "precisions = []\n",
    "average_precisions = []\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "classifiers = []\n",
    "\n",
    "# X_transf = rfecv.transform(X)\n",
    "X_transf = X.copy().values\n",
    "# X_transf = X.copy().values\n",
    "# X.reset_index(inplace=True, drop=True)\n",
    "y.reset_index(inplace=True, drop=True)\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X_transf, y)):\n",
    "#     clf = RandomForestClassifier(n_estimators=100, max_features=.2, min_samples_leaf=17, min_samples_split=9, bootstrap=False, criterion=\"gini\", class_weight=\"balanced_subsample\")\n",
    "#     clf = ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='entropy', max_features=0.8, min_samples_leaf=4, min_samples_split=20, n_estimators=100)\n",
    "    clf = make_pipeline(\n",
    "            MinMaxScaler(),\n",
    "            RFECV(LogisticRegression(penalty=\"l1\", solver=\"liblinear\", fit_intercept=True, dual=False, tol=0.001, class_weight=\"balanced\"), step=1, cv=StratifiedKFold(random_state=930525, shuffle=True), scoring=\"f1\", min_features_to_select=5, n_jobs=40),\n",
    "            MLPClassifier(alpha=0.01, learning_rate_init=0.01)\n",
    "        )\n",
    "\n",
    "#     clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", fit_intercept=True, dual=False, tol=0.001, class_weight=\"balanced\")\n",
    "    clf.fit(X_transf[train], y.loc[train])\n",
    "    classifiers.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    joblib.dump(classifier, f'../data/clf.sklearn.{categories[i]}.{depth}.pkl', compress=9)\n",
    "    \n",
    "joblib.dump(X, f'../data/X.{depth}.pkl', compress=9)\n",
    "joblib.dump(y, f'../data/y{depth}.pkl', compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "columns = ['depth', 'dataset', 'precision', 'recall', 'f1', 'ap', 'tp', 'tn', 'fp', 'fn']\n",
    "rows = []\n",
    "for i, ((train, test), classifier) in enumerate(zip(cv.split(X_transf, y), classifiers)):\n",
    "    viz = plot_precision_recall_curve(classifier, X_transf[test], y.loc[test],\n",
    "                     name=f'{categories[i]}',\n",
    "                     alpha=0.3, lw=1, ax=ax)\n",
    "    interp_precision = np.interp(mean_recall, viz.recall[::-1], viz.precision[::-1])\n",
    "    interp_precision[0] = 1.0\n",
    "    precisions.append(interp_precision)\n",
    "    average_precisions.append(viz.average_precision)\n",
    "    y_pred = classifier.predict(X_transf[test])\n",
    "    print(f\"Dataset: {categories[i]}\")\n",
    "    print(f\"F1: {f1_score(y.loc[test], y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y.loc[test], y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y.loc[test], y_pred)}\")\n",
    "    print(f\"Num Predicted: {y_pred.sum()}\")\n",
    "    \n",
    "    row = [\n",
    "        depth,\n",
    "        categories[i],\n",
    "        precision_score(y.loc[test], y_pred),\n",
    "        recall_score(y.loc[test], y_pred),\n",
    "        f1_score(y.loc[test], y_pred),\n",
    "        viz.average_precision,\n",
    "        (y_pred[y.loc[test]]).sum(),\n",
    "        (~y_pred[~y.loc[test]]).sum(),\n",
    "        (y_pred[~y.loc[test]]).sum(),\n",
    "        (~y_pred[y.loc[test]]).sum()\n",
    "    ]\n",
    "    rows.append(row)\n",
    "\n",
    "mean_precision = np.mean(precisions, axis=0)\n",
    "mean_precision[-1] = 0.0\n",
    "mean_average_precisions = np.mean(average_precisions)\n",
    "std_average_precisions = np.std(average_precisions)\n",
    "\n",
    "ax.plot(mean_recall, mean_precision, color='b',\n",
    "        label=r'Mean PR (AP = %0.2f $\\pm$ %0.2f)' % (mean_average_precisions, std_average_precisions),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "# calculate the no skill line as the proportion of the positive class\n",
    "# no_skill = len(y[y==False]) / len(y)\n",
    "# # plot the no skill precision-recall curve\n",
    "# plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label=f'No Skill ( AP = {no_skill:.5f})')\n",
    "\n",
    "std_precisions = np.std(precisions, axis=0)\n",
    "precisions_upper = np.minimum(mean_precision + std_precisions, 1)\n",
    "precisions_lower = np.maximum(mean_precision - std_precisions, 0)\n",
    "ax.fill_between(mean_recall, precisions_lower, precisions_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Precision Recall Curves\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "\n",
    "df_results = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "df_results.to_csv(f\"../data/results.{depth}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-type_1]",
   "language": "python",
   "name": "conda-env-.conda-type_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
