{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas    : 1.1.5\n",
      "seaborn   : 0.11.1\n",
      "matplotlib: 3.3.4\n",
      "numpy     : 1.20.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(930525)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "warnings.simplefilter('once')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/tpot-cuml/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/tpot-cuml/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/bhillmann/.conda/envs/tpot-cuml/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3156: DtypeWarning: Columns (73,86,98,99,103,106,107,111,112,204,217,229,230,234,237,238,242,243) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_type_1_features = pd.read_csv(\"../data/strains.dataset.tree.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_1_features['dataset_cat'] = pd.Series([_.split(\"_\")[0] for _ in df_type_1_features['dataset']], dtype='category')\n",
    "CV = PredefinedSplit(df_type_1_features['dataset_cat'].cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['hits',\n",
    " 'percent_coverage',\n",
    " 'mean_coverage',\n",
    " 'sd_coverage',\n",
    " 'percent_binned_coverage',\n",
    " 'mean_binned_coverage',\n",
    " 'sd_binned_coverage',\n",
    " 'expected_percent_coverage',\n",
    " 'shannon_entropy',\n",
    " 'percent_max_uncovered_region',\n",
    " 'largest_pileup',\n",
    " 'largest_binned_pileup',\n",
    " 'gc_content',\n",
    " 'total_genome_length',\n",
    " 'ungapped_genome_length',\n",
    " 'num_n_groups',\n",
    " 'consecutive_ns',\n",
    " 'tree_dist',\n",
    " 'tree_top_dist',\n",
    " 'gf_checkm_completeness',\n",
    " 'gf_checkm_contamination'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhillmann/.conda/envs/tpot-cuml/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_type_1 = df_type_1_features[features + [\"assembly_accession\", \"dataset\", \"truth\", \"dataset_cat\"]]\n",
    "                                                 \n",
    "X_type_1 = X_type_1.replace([np.inf, -np.inf], np.nan)\n",
    "X_type_1 = X_type_1.dropna()\n",
    "\n",
    "y = X_type_1[\"truth\"]\n",
    "X = X_type_1.loc[:, X_type_1.columns.difference([\"assembly_accession\", \"dataset\", \"dataset_cat\", \"truth\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-88fe7b62ec8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tpot-cuml/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6523\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6524\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6525\u001b[0;31m             \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6526\u001b[0m         )\n\u001b[1;32m   6527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tpot-cuml/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             )\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tpot-cuml/lib/python3.7/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_features=.2, min_samples_leaf=17, min_samples_split=9, bootstrap=False, criterion=\"gini\")\n",
    "\n",
    "logistic = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", fit_intercept=True, dual=False, tol=0.001)\n",
    "\n",
    "min_features_to_select = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv = RFECV(rfc, step=1, cv=CV, scoring=\"f1\", min_features_to_select=min_features_to_select, n_jobs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[rfecv.ranking_ == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(min_features_to_select,\n",
    "               len(rfecv.grid_scores_) + min_features_to_select),\n",
    "         rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_type_1_features['dataset_cat'] = pd.Series([_.split(\"_\")[0] for _ in df_type_1_features['dataset']], dtype='category')\n",
    "cv = PredefinedSplit(df_type_1_features['dataset_cat'].cat.codes)\n",
    "classifier = rfc\n",
    "\n",
    "categories = df_type_1_features['dataset_cat'].cat.categories\n",
    "\n",
    "X = X_type_1.loc[:, X_type_1.columns.difference([\"assembly_accession\", \"dataset\", \"dataset_cat\", \"truth\"])]\n",
    "X = rfecv.transform(X)\n",
    "\n",
    "precisions = []\n",
    "average_precisions = []\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "classifiers = []\n",
    "\n",
    "for i, (train, test) in enumerate(CV.split(X, y)):\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_features=.2, min_samples_leaf=17, min_samples_split=9, bootstrap=False, criterion=\"gini\")\n",
    "    clf.fit(X[train], y[train])\n",
    "    classifiers.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i, ((train, test), classifier) in enumerate(zip(CV.split(X, y), classifiers)):\n",
    "    viz = plot_precision_recall_curve(classifier, X[test], y[test],\n",
    "                     name=f'{categories[i]}',\n",
    "                     alpha=0.3, lw=1, ax=ax)\n",
    "    interp_precision = np.interp(mean_recall, viz.recall[::-1], viz.precision[::-1])\n",
    "    interp_precision[0] = 1.0\n",
    "    precisions.append(interp_precision)\n",
    "    average_precisions.append(viz.average_precision)\n",
    "    y_pred = classifier.predict(X[test])\n",
    "    print(f1_score(y[test], y_pred))\n",
    "    \n",
    "\n",
    "mean_precision = np.mean(precisions, axis=0)\n",
    "mean_precision[-1] = 0.0\n",
    "mean_average_precisions = np.mean(average_precisions)\n",
    "std_average_precisions = np.std(average_precisions)\n",
    "\n",
    "ax.plot(mean_recall, mean_precision, color='b',\n",
    "        label=r'Mean PR (AP = %0.2f $\\pm$ %0.2f)' % (mean_average_precisions, std_average_precisions),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "# calculate the no skill line as the proportion of the positive class\n",
    "no_skill = len(y[y==True]) / len(y)\n",
    "# plot the no skill precision-recall curve\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label=f'No Skill ( AP = {no_skill:.5f})')\n",
    "\n",
    "std_precisions = np.std(precisions, axis=0)\n",
    "precisions_upper = np.minimum(mean_precision + std_precisions, 1)\n",
    "precisions_lower = np.maximum(mean_precision - std_precisions, 0)\n",
    "ax.fill_between(mean_recall, precisions_lower, precisions_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Precision Recall Curves\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import the required library\n",
    "# from numpy import argmax\n",
    "\n",
    "# # Calculate F-Scores and find the index of ideal score\n",
    "# fscore = (2 * mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
    "# ix = argmax(fscore)\n",
    "# best_thresh = thresholds[ix]\n",
    "# print('Best Threshold: %f' % (best_thresh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tpot-cuml]",
   "language": "python",
   "name": "conda-env-.conda-tpot-cuml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
